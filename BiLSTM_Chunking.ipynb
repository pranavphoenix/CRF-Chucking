{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full code Chunking BiLSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iih2mxCj1FLT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "749f2018-3856-482a-8141-36e008d1f8c3"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "import pickle\n",
        "import os,sys\n",
        "from io import open\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import brown\n",
        "\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "train = open(\"train.txt\", \"r\")\n",
        "test = open(\"test.txt\", \"r\")\n",
        "train_lines = train.readlines()\n",
        "test_lines = test.readlines()\n",
        "\n",
        "train_sentences  = [ [] for _ in range(8936) ]\n",
        "train_pos = [ [] for _ in range(8936) ]\n",
        "train_chunks = [ [] for _ in range(8936) ]\n",
        "j = 0\n",
        "for line in train_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    train_sentences[j].append(line.split()[0])\n",
        "    train_chunks[j].append(line.split()[2].split('-')[0])\n",
        "\n",
        "test_sentences  = [ [] for _ in range(2012) ]\n",
        "test_chunks = [ [] for _ in range(2012) ]\n",
        "j = 0\n",
        "for line in test_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    test_sentences[j].append(line.split()[0])\n",
        "    test_chunks[j].append(line.split()[2].split('-')[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-03 19:41:35--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-10-03 19:41:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-10-03 19:41:36--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.04MB/s    in 6m 27s  \n",
            "\n",
            "2020-10-03 19:48:03 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVixs48wUhgV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "12d9c18d-21f4-4884-8857-ddf52e1b0752"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "import pickle\n",
        "import os,sys\n",
        "from io import open\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed\n",
        "from keras.layers import Embedding, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import brown\n",
        "\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "train = open(\"train.txt\", \"r\")\n",
        "test = open(\"test.txt\", \"r\")\n",
        "train_lines = train.readlines()\n",
        "test_lines = test.readlines()\n",
        "\n",
        "train_sentences  = [ [] for _ in range(8936) ]\n",
        "train_pos = [ [] for _ in range(8936) ]\n",
        "train_chunks = [ [] for _ in range(8936) ]\n",
        "j = 0\n",
        "for line in train_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    train_sentences[j].append(line.split()[0])\n",
        "    train_chunks[j].append(line.split()[2].split('-')[0])\n",
        "\n",
        "test_sentences  = [ [] for _ in range(2012) ]\n",
        "test_chunks = [ [] for _ in range(2012) ]\n",
        "j = 0\n",
        "for line in test_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    test_sentences[j].append(line.split()[0])\n",
        "    test_chunks[j].append(line.split()[2].split('-')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCY_YjzoGRvv"
      },
      "source": [
        "train_pos = [ [] for _ in range(8936) ]\n",
        "test_pos = [ [] for _ in range(2012) ]\n",
        "j = 0\n",
        "for line in train_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    train_pos[j].append(line.split()[1])\n",
        "j = 0\n",
        "for line in test_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    test_pos[j].append(line.split()[1])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV0V79RvOpqi"
      },
      "source": [
        "test_sentences  = [ [] for _ in range(2012) ]\n",
        "test_chunks = [ [] for _ in range(2012) ]\n",
        "j = 0\n",
        "for line in test_lines:\n",
        "  if line == '\\n':\n",
        "    j = j+1\n",
        "  else:\n",
        "    test_sentences[j].append(line.split()[0])\n",
        "    test_chunks[j].append(line.split()[2].split('-')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKdzQ15g2Seg"
      },
      "source": [
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "def one_hot_encoding(tag_sents, n_tags):\n",
        "    tag_one_hot_sent = []\n",
        "    for tag_sent in tag_sents:\n",
        "        tags_one_hot = []\n",
        "        for tag in tag_sent:\n",
        "            tags_one_hot.append(np.zeros(n_tags))\n",
        "            tags_one_hot[-1][tag] = 1.0\n",
        "        tag_one_hot_sent.append(tags_one_hot)\n",
        "    return np.array(tag_one_hot_sent)\n",
        "\n",
        "def logits_to_tags(tag_sentences, index):\n",
        "    tag_sequences = []\n",
        "    for tag_sentence in tag_sentences:\n",
        "        tag_sequence = []\n",
        "        for tag in tag_sentence:\n",
        "            # if index[np.argmax(tag)] == \"-PAD-\":\n",
        "            #     break\n",
        "            # else:\n",
        "                tag_sequence.append(index[np.argmax(tag)])\n",
        "        tag_sequences.append(np.array(tag_sequence))\n",
        "    return tag_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNhPvQUW2wHm"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences, key=len))\n",
        "\n",
        "\n",
        "chunk_list=['-PAD-','B','I','O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enC11K1EPzaL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34f5a978-d497-4307-a9f8-2447a2edc7ca"
      },
      "source": [
        "embeddings_index = {} #initialize dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhZO6aDcKFJJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "2e1f0a22-b970-4aed-901f-100c91fc4469"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, Bidirectional, TimeDistributed\n",
        "from keras.optimizers import Adagrad\n",
        "\n",
        "words, chunks = set([]), set([])\n",
        "tags = set([])\n",
        "    #creating sets of words and tags \n",
        "for sentence in train_sentences:\n",
        "  for word in sentence:\n",
        "    words.add(word.lower())\n",
        " \n",
        "for sentence in train_chunks:\n",
        "  for chunk in sentence:\n",
        "    chunks.add(chunk)\n",
        "\n",
        "for tag_sent in train_pos:\n",
        "        for tag in tag_sent:\n",
        "            tags.add(tag)\n",
        "\n",
        "\n",
        "    #bulding vocabulary of words and tags \n",
        "word2index = {word: i + 2 for i, word in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "word2index['-OOV-'] = 1  # 1 is assigned for unknown words\n",
        "chunk2index = {chunk: i + 1 for i, chunk in enumerate(list(chunks))}\n",
        "chunk2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "tag2index = {tag: i + 1 for i, tag in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # 0 is assigned for padding\n",
        "\n",
        "    #Tokenising words and  by their indexes in vocabulary\n",
        "train_sentences_X, test_sentences_X, train_chunks_y, test_chunks_y = [], [], [], []\n",
        "train_pos_X, test_pos_X = [], []\n",
        " \n",
        "for sentence in train_sentences:\n",
        "  sent_int = []\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      sent_int.append(word2index[word.lower()])\n",
        "    except KeyError:\n",
        "      sent_int.append(word2index['-OOV-'])\n",
        "  train_sentences_X.append(sent_int)\n",
        " \n",
        "for sentence in test_sentences:\n",
        "  sent_int = []\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      sent_int.append(word2index[word.lower()])\n",
        "    except KeyError:\n",
        "      sent_int.append(word2index['-OOV-'])\n",
        "  test_sentences_X.append(sent_int)\n",
        " \n",
        "for sent_chunks in train_chunks:\n",
        "  train_chunks_y.append([chunk2index[chunk] for chunk in sent_chunks])\n",
        " \n",
        "for sent_chunks in test_chunks:\n",
        "  test_chunks_y.append([chunk2index[chunk] for chunk in sent_chunks])\n",
        "\n",
        "for sent_tags in train_pos:\n",
        "  train_pos_X.append([tag2index[tag] for tag in sent_tags])\n",
        " \n",
        "for sent_tags in test_pos:\n",
        "  test_pos_X.append([tag2index[tag] for tag in sent_tags])  \n",
        "\n",
        "    #Add padding to sentences\n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_chunks_y = pad_sequences(train_chunks_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_chunks_y = pad_sequences(test_chunks_y, maxlen=MAX_LENGTH, padding='post')\n",
        "train_pos_X = pad_sequences(train_pos_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_pos_X = pad_sequences(test_pos_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "    #Building the Embedding Layer \n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2index), embedding_dim))\n",
        "for word, i in word2index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if i < len(word2index):\n",
        "    if embedding_vector is not None:\n",
        "                # Words not found in embedding index will be all-zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "# Building the BiLSTM model without POS tag\n",
        "# model = Sequential()\n",
        "# model.add(InputLayer(input_shape=MAX_LENGTH,))\n",
        "# model.add(Embedding(len(word2index), 300, weights=[embedding_matrix],trainable=False))\n",
        "# model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "# model.add(TimeDistributed(Dense(len(chunk2index))))\n",
        "# model.add(Activation('softmax'))\n",
        "\n",
        "#Building the BiLSTM model with POS tag\n",
        "word_input = Input(\n",
        "    shape=(MAX_LENGTH,), name=\"word\"\n",
        ")  # text\n",
        "tags_input = Input(\n",
        "    shape=(MAX_LENGTH,), name=\"tags\"\n",
        ")  # tags\n",
        "\n",
        "tag_features = Embedding(len(tag2index), 128)(tags_input)\n",
        "\n",
        "word_features = Embedding(len(word2index), 300, weights=[embedding_matrix],trainable=False)(word_input)\n",
        "word_lstm = Bidirectional(LSTM(256, return_sequences=True))(word_features)\n",
        "tag_lstm = Bidirectional(LSTM(256, return_sequences=True))(tag_features)\n",
        "\n",
        "x = concatenate([word_lstm, tag_lstm])\n",
        "o = Dense(len(chunk2index))(x)\n",
        "output = Activation('softmax')(o)\n",
        "\n",
        "model = Model(inputs=[word_input, tags_input],outputs= output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),\n",
        "                  metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "model.summary()\n",
        "one_hot_train_chuns_y = one_hot_encoding(train_chunks_y, len(chunk2index))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "word (InputLayer)               [(None, 78)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tags (InputLayer)               [(None, 78)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_21 (Embedding)        (None, 78, 300)      5178000     word[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_20 (Embedding)        (None, 78, 128)      5760        tags[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_27 (Bidirectional (None, 78, 512)      1140736     embedding_21[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_28 (Bidirectional (None, 78, 512)      788480      embedding_20[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 78, 1024)     0           bidirectional_27[0][0]           \n",
            "                                                                 bidirectional_28[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 78, 4)        4100        concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 78, 4)        0           dense_10[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 7,117,076\n",
            "Trainable params: 1,939,076\n",
            "Non-trainable params: 5,178,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czu28_TPO6Gx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "42cd5a8d-d771-4d2f-b08f-ecb03f19d790"
      },
      "source": [
        "#Training the model\n",
        "\n",
        "# model.fit({\"word\":train_sentences_X, \"tags\": train_pos_X}, one_hot_encoding(train_chunks_y, len(chunk2index)), \n",
        "#               batch_size=128, epochs= 13, validation_split=0.2)  #with POS tag\n",
        "\n",
        "#without POS tag\n",
        "model.fit(train_sentences_X, one_hot_encoding(train_chunks_y, len(chunk2index)), batch_size=128, epochs= 20, validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "56/56 [==============================] - 3s 49ms/step - loss: 0.2841 - accuracy: 0.8984 - ignore_accuracy: 0.7164 - val_loss: 0.1321 - val_accuracy: 0.9447 - val_ignore_accuracy: 0.8189\n",
            "Epoch 2/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.1131 - accuracy: 0.9547 - ignore_accuracy: 0.8514 - val_loss: 0.1039 - val_accuracy: 0.9592 - val_ignore_accuracy: 0.8663\n",
            "Epoch 3/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0921 - accuracy: 0.9653 - ignore_accuracy: 0.8860 - val_loss: 0.0895 - val_accuracy: 0.9667 - val_ignore_accuracy: 0.8909\n",
            "Epoch 4/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0782 - accuracy: 0.9710 - ignore_accuracy: 0.9048 - val_loss: 0.0787 - val_accuracy: 0.9702 - val_ignore_accuracy: 0.9024\n",
            "Epoch 5/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0679 - accuracy: 0.9753 - ignore_accuracy: 0.9189 - val_loss: 0.0705 - val_accuracy: 0.9735 - val_ignore_accuracy: 0.9133\n",
            "Epoch 6/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0601 - accuracy: 0.9783 - ignore_accuracy: 0.9286 - val_loss: 0.0650 - val_accuracy: 0.9756 - val_ignore_accuracy: 0.9202\n",
            "Epoch 7/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0533 - accuracy: 0.9813 - ignore_accuracy: 0.9385 - val_loss: 0.0599 - val_accuracy: 0.9781 - val_ignore_accuracy: 0.9284\n",
            "Epoch 8/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0487 - accuracy: 0.9831 - ignore_accuracy: 0.9446 - val_loss: 0.0580 - val_accuracy: 0.9784 - val_ignore_accuracy: 0.9293\n",
            "Epoch 9/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0438 - accuracy: 0.9851 - ignore_accuracy: 0.9514 - val_loss: 0.0543 - val_accuracy: 0.9805 - val_ignore_accuracy: 0.9362\n",
            "Epoch 10/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0397 - accuracy: 0.9868 - ignore_accuracy: 0.9568 - val_loss: 0.0525 - val_accuracy: 0.9809 - val_ignore_accuracy: 0.9373\n",
            "Epoch 11/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0357 - accuracy: 0.9883 - ignore_accuracy: 0.9617 - val_loss: 0.0519 - val_accuracy: 0.9813 - val_ignore_accuracy: 0.9387\n",
            "Epoch 12/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0325 - accuracy: 0.9895 - ignore_accuracy: 0.9657 - val_loss: 0.0503 - val_accuracy: 0.9820 - val_ignore_accuracy: 0.9412\n",
            "Epoch 13/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0294 - accuracy: 0.9906 - ignore_accuracy: 0.9693 - val_loss: 0.0503 - val_accuracy: 0.9824 - val_ignore_accuracy: 0.9423\n",
            "Epoch 14/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0265 - accuracy: 0.9919 - ignore_accuracy: 0.9735 - val_loss: 0.0504 - val_accuracy: 0.9822 - val_ignore_accuracy: 0.9418\n",
            "Epoch 15/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0235 - accuracy: 0.9929 - ignore_accuracy: 0.9768 - val_loss: 0.0511 - val_accuracy: 0.9822 - val_ignore_accuracy: 0.9417\n",
            "Epoch 16/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0204 - accuracy: 0.9941 - ignore_accuracy: 0.9809 - val_loss: 0.0511 - val_accuracy: 0.9825 - val_ignore_accuracy: 0.9429\n",
            "Epoch 17/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0179 - accuracy: 0.9950 - ignore_accuracy: 0.9838 - val_loss: 0.0527 - val_accuracy: 0.9824 - val_ignore_accuracy: 0.9425\n",
            "Epoch 18/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0150 - accuracy: 0.9961 - ignore_accuracy: 0.9873 - val_loss: 0.0530 - val_accuracy: 0.9827 - val_ignore_accuracy: 0.9434\n",
            "Epoch 19/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0126 - accuracy: 0.9970 - ignore_accuracy: 0.9904 - val_loss: 0.0539 - val_accuracy: 0.9827 - val_ignore_accuracy: 0.9432\n",
            "Epoch 20/20\n",
            "56/56 [==============================] - 2s 35ms/step - loss: 0.0105 - accuracy: 0.9976 - ignore_accuracy: 0.9924 - val_loss: 0.0563 - val_accuracy: 0.9824 - val_ignore_accuracy: 0.9425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9d4f7c2518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs9h0SsTPfuh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e477f948-77d0-480b-bc7c-7259c24e9efe"
      },
      "source": [
        "#scores = model.evaluate({\"word\":test_sentences_X, \"tags\": test_pos_X}, one_hot_encoding(test_chunks_y, len(chunk2index)))\n",
        "scores = model.evaluate(test_sentences_X, one_hot_encoding(test_chunks_y, len(chunk2index)))\n",
        "scores[2]*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 1s 9ms/step - loss: 0.0594 - accuracy: 0.9818 - ignore_accuracy: 0.9399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.9885139465332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9RsCJ4VRxj8"
      },
      "source": [
        "# predictions = model.predict({\"word\":test_sentences_X, \"tags\": test_pos_X})\n",
        "predictions = model.predict(test_sentences_X)\n",
        "pred_sequence = logits_to_tags(predictions, {i: t for t, i in chunk2index.items()})\n",
        "\n",
        "\n",
        "conf_mat_df = pd.DataFrame(columns=chunk_list, index=chunk_list)\n",
        "conf_mat_df = conf_mat_df.fillna(0)\n",
        "\n",
        "for sen_num in range(len(test_chunks)):\n",
        "  for i,chunk in enumerate(test_chunks[sen_num]):\n",
        "    conf_mat_df[chunk][pred_sequence[sen_num][i]] +=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5HBZ9PHVRkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "48f15155-a83d-4cd7-80fe-cd1218a1cfbd"
      },
      "source": [
        "conf_mat_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-PAD-</th>\n",
              "      <th>B</th>\n",
              "      <th>I</th>\n",
              "      <th>O</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>-PAD-</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>0</td>\n",
              "      <td>22834</td>\n",
              "      <td>1203</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>0</td>\n",
              "      <td>908</td>\n",
              "      <td>15836</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>0</td>\n",
              "      <td>104</td>\n",
              "      <td>302</td>\n",
              "      <td>5844</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       -PAD-      B      I     O\n",
              "-PAD-      0      6      4     5\n",
              "B          0  22834   1203   151\n",
              "I          0    908  15836   180\n",
              "O          0    104    302  5844"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRbu2zlbeigx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "5aaaf8c7-698d-4030-9334-8637c62aa959"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(conf_mat_df, cmap=\"YlGnBu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d57dc0d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB8M6J7JYRPg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "133dc32d-8632-4b1d-9617-6a8f4d72eb23"
      },
      "source": [
        "recall_B = conf_mat_df['B']['B']/(conf_mat_df['B']['B']+conf_mat_df['B']['I'])\n",
        "print(\"recall_B:\" +  str(recall_B))\n",
        "recall_I = conf_mat_df['I']['I']/(conf_mat_df['I']['B']+conf_mat_df['I']['I'])\n",
        "print(\"recall_I:\" + str(recall_I))\n",
        "\n",
        "total_recall = (conf_mat_df['B']['B'] + conf_mat_df['I']['I'])/(conf_mat_df['B']['B']+conf_mat_df['B']['I']+conf_mat_df['I']['B']+conf_mat_df['I']['I'])\n",
        "print(\"recall:\" +  str(total_recall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recall_B:0.9617555387077753\n",
            "recall_I:0.929397265097717\n",
            "recall:0.9482356979966161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxMXuDlxa2kj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "21d57bc5-7d8f-40eb-b09e-72c815817bfb"
      },
      "source": [
        "precision_B = conf_mat_df['B']['B']/(conf_mat_df['B']['B']+conf_mat_df['I']['B'])\n",
        "print(\"precision_B:\" +  str(precision_B))\n",
        "precision_I = conf_mat_df['I']['I']/(conf_mat_df['B']['I']+conf_mat_df['I']['I'])\n",
        "print(\"precision_I:\" + str(precision_I))\n",
        "\n",
        "total_precision = (conf_mat_df['B']['B'] + conf_mat_df['I']['I'])/(conf_mat_df['B']['B']+conf_mat_df['B']['I']+conf_mat_df['I']['B']+conf_mat_df['I']['I'])\n",
        "print(\"precision:\" +  str(total_precision))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision_B:0.9499521570911511\n",
            "precision_I:0.9457716196846632\n",
            "precision:0.9482356979966161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfBof2YJcYSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a6e76119-e2f1-4c69-d149-3bfa40ba3b56"
      },
      "source": [
        "f1score_B = 2*precision_B*recall_B/ (recall_B +precision_B)\n",
        "f1score_I = 2*precision_I*recall_I/ (recall_I +precision_I)\n",
        "print(\"f1score B:\" + str(f1score_B))\n",
        "print(\"f1score I:\" + str(f1score_I))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1score B:0.9558174093220873\n",
            "f1score I:0.937512950300447\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}